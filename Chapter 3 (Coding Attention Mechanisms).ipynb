{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59304519-5739-48d1-9af2-c23a44e632c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple self-attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts (x^3)\n",
    "     [0.22, 0.58, 0.33], # with (x^4)\n",
    "     [0.77, 0.25, 0.10], # one (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c826d627-c750-4e71-bc26-ce68fdabec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1] # Assuming journey as the word\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae97675c-9ab6-4009-9268-08bc30c2ca72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8b9f05-f41a-4951-b0d3-1f995911053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.55*0.43 + 0.87*0.15 + 0.66*0.89 #Manual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690239d0-a31f-402b-8d51-59b7e7d22f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1) #Pytorch commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d42356c-abf4-40e6-a4e3-1fc93ed9ede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4300)\n",
      "tensor(0.1500)\n",
      "tensor(0.8900)\n"
     ]
    }
   ],
   "source": [
    "for idx,ele in enumerate(inputs[0]):\n",
    "    print(inputs[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f0fcf7-33c5-4f84-9e77-9bb60d900c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0\n",
    "for idx,ele in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * input_query[idx]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1109186a-2910-4534-be78-a9652f5ad780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0\n",
    "for idx,ele in enumerate(inputs[1]):\n",
    "    res += inputs[1][idx] * input_query[idx]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1062cbfd-6ac0-420f-86f3-30c17723c56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8434)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "res = torch.dot(inputs[i], input_query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05cf3f2-4166-45df-b5b7-517d575085db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436b3632-e23e-4d81-8f31-74e5474c4687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee20a9a7-ddb1-4390-aa6f-c5b6e9045b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "#automating for all rows calculating the attention scores\n",
    "query = inputs[1] #2nd input token is the query\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, input_query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa12a92-a35e-46ea-bd53-2859884af3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing them\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "attn_weights_2_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7499077-3a33-4fc0-8dd7-56b519942042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2_tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9109bfca-bb02-4529-92db-f01699bc47e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "softmax_naive(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83f29bc-cd59-418a-975f-760584af9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the generally used way instead of making manual function\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95b00fa4-b921-44e5-97f0-fbebdb0edd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    print(i, inputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231aba42-f9a6-4491-919e-a4060ffa9d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13854756951332092 ---> tensor([0.4300, 0.1500, 0.8900])\n",
      "0.2378913015127182 ---> tensor([0.5500, 0.8700, 0.6600])\n",
      "0.23327402770519257 ---> tensor([0.5700, 0.8500, 0.6400])\n",
      "0.12399158626794815 ---> tensor([0.2200, 0.5800, 0.3300])\n",
      "0.10818186402320862 ---> tensor([0.7700, 0.2500, 0.1000])\n",
      "0.15811361372470856 ---> tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #2nd input taken as the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(f\"{attn_weights_2[i]} ---> {inputs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6376ceb3-cd53-41de-a91f-8542381f8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #2nd input taken as the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*inputs[i]\n",
    "print (context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f753ee-6900-41bb-8702-2a0c3b542c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts (x^3)\n",
    "     [0.22, 0.58, 0.33], # with (x^4)\n",
    "     [0.77, 0.25, 0.10], # one (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8b6f43-3c3a-490e-ba49-ed8aef597094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Continuation of previous section but unlike focusing on one query it will be generalized\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d75b3740-058f-4a29-9d44-10e09a7c1367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternative way using matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebd41831-69b1-40b9-9316-251bcfacc1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating attention weights from attention scores by normalizing them\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efd6161-9ef0-4a1c-b4b7-6e3faf6cf1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99ca9d66-b395-44f1-b8f1-b25cbd772b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can see all we have done till now in a very small reduced code version\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c0f5d1-fd92-48ab-8a51-1cc0726e7006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd34b6b-2dca-41b7-a113-2d2115873f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so now the final code with all work done from start to end\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21e03de8-97f6-4fad-a03a-ecb16408c4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Self atetention with trainable weights\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1082ead-4e02-4a4e-800f-810dcdc0e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42acec90-1554-4a9f-b8d6-e1819467de2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39bb0ae2-6167-4053-a888-11218e10286c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a718af9-506e-4666-9a2b-e3fad0083f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_query #requires training unfortunately which will come in next parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afd468f5-6ba7-45f0-be20-a055e1080a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b73b955-9057-4aed-85ad-0f88b18c7e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20020c6e-92c3-4eef-a323-571a611159c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys  = inputs @ W_key\n",
    "value = inputs @ W_value\n",
    "\n",
    "keys.shape\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64a47e47-9372-4e91-a404-5ecd4740d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "085c38e0-6a11-457b-bb7a-a6d6a5d285a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1c84b9d-94ca-42db-b98e-8fdfb2b31eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = torch.dot(query_2, keys_2)\n",
    "attn_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0163cc07-b8f0-402d-86bf-8405d7111b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_2 = query_2 @ keys.T\n",
    "attn_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9fe304f-ada3-4d11-bda9-fa70a0e57af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weight_2 = torch.softmax(attn_score_2 / d_k**0.5, dim=-1)\n",
    "attn_weight_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "595a40fc-374c-427b-8edb-2ece01e3d155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(attn_weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99165bff-bc29-403d-8b72-b3c3b34eb205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weight_2 @ value\n",
    "\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37a0e7ba-9ba6-4334-91f3-701716998970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a compact self attention class\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = inputs @ W_query\n",
    "        keys = inputs @ W_key\n",
    "        values = inputs @ W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/ d_k**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "184369ce-54e5-4030-855e-926da294f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17d2756c-1472-4942-8c8e-0f2b026e1af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f08bd1e-1df8-4a17-85d2-ce0b71021745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a compact self attention class version 2\n",
    "#Gives better weight initialization\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(inputs)\n",
    "        keys = self.W_key(inputs)\n",
    "        values = self.W_value(inputs)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/ d_k**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b70b459-0e36-4e23-8011-be5561ee47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41ce2f8b-538c-4ba8-ad4e-072ab6ee12bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2(inputs) #Different output because different way of weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27e45680-7da7-4c0d-b17e-2b78fddaf840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiding future words with causal attention\n",
    "#Modification to self attention mechanism to avoid future words\n",
    "#Applying a causal attention mask\n",
    "#Your journey starts with one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d55c652c-d453-4f56-91fa-2dc842ba40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "values = sa_v2.W_value(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/ d_k**0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b9f27b4-580d-42c0-aa0d-6f64561ec9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
       "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
       "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c71a38a4-13b0-4aad-880a-ad5634a7d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c38d3e98-374f-499d-b16e-64c840290cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd55cc27-0415-4142-8a1c-1dbd34a16d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7fb7e5e-e4f5-4140-a363-f21ee774d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Another way to normalize\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "447e78d6-6516-4f99-a9b8-b4fb5d0d0bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(-999999999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7743e30-ede7-486e-b9ac-3d503a8dbec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(float(\"-inf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "954b4517-1444-4b43-a6bb-79eed7da380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / d_k**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f60c589-24e9-4871-ae4a-1edcac126401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking additional attention weights with dropout\n",
    "#Used to remove overfitting so model relies rest on certain positions\n",
    "\n",
    "torch.manual_seed(123)\n",
    "layer = torch.nn.Dropout(0.5) #0.5 means drops 50% of positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3eec6556-7851-4ef1-a0c8-1a38c1195c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.ones(6,6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b66ae96-e894-4b9d-9f37-893b3f1567c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86379d64-22e5-4a36-9148-1a142f340d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.5\n",
    "1 / (1-dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76d2525d-f5e4-499e-ba06-8afd0fe23dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
       "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "977eacbc-7fbe-4a0b-b725-c497715e1e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing a compact causal self attention class\n",
    "\n",
    "batch = torch.stack((inputs,inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8402445c-7166-4037-9095-3957c22de82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # Correctly compute attention scores via matrix multiplication\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        \n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask[:num_tokens, :num_tokens].bool(), -torch.inf)\n",
    "            \n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)  \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a36010c-c6f4-4345-8be5-99996f464a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de82ca88-8407-470e-b0b1-f5b29c2014b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4df68ce-0fc1-454e-a31b-55519c9c7ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0872,  0.0286],\n",
       "         [-0.0991,  0.0501],\n",
       "         [-0.0999,  0.0633],\n",
       "         [-0.0983,  0.0489],\n",
       "         [-0.0514,  0.1098],\n",
       "         [-0.0754,  0.0693]],\n",
       "\n",
       "        [[-0.0872,  0.0286],\n",
       "         [-0.0991,  0.0501],\n",
       "         [-0.0999,  0.0633],\n",
       "         [-0.0983,  0.0489],\n",
       "         [-0.0514,  0.1098],\n",
       "         [-0.0754,  0.0693]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "context_length = batch.shape[1]\n",
    "dropout = 0.0\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout)\n",
    "ca(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68f2e066-5808-4bda-9eb8-abf183eb6b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4115b98a-a7a9-4c1a-9ce7-b7926f85b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extending single head attention to multi head attention\n",
    "\n",
    "#stacking multiple single head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40479161-550c-420d-b954-841eeb8bfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads=2, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList({\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb3f93be-bfe6-4d57-b8f1-bb1e5061ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3,2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
    "mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1032ee39-9fa5-48f0-9420-dd1c8056c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing multi head attention with weight splits\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert( d_out % num_heads == 0 ), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        #Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        #We implicitly split the matrix by adding a num_heads dimension\n",
    "        #Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        #Compute scaled dot product attention (aka self attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3) #Dot product for each head\n",
    "        #Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        #Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        #Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        #Combine heads, where self.d_out = elf.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) #Optional Projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd168250-1475-4740-940c-437ab4e56396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
      "\n",
      "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
